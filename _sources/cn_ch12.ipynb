{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mplfonts.bin.cli import init\n",
    "init()\n",
    "from mplfonts import use_font\n",
    "use_font('SimHei')\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 一些有用的工具函数\n",
    "\n",
    "def laplace_mech(v, sensitivity, epsilon):\n",
    "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
    "\n",
    "def gaussian_mech(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
    "\n",
    "def gaussian_mech_vec(v, sensitivity, epsilon, delta):\n",
    "    return v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon, size=len(v))\n",
    "\n",
    "def pct_error(orig, priv):\n",
    "    return np.abs(orig - priv)/orig * 100.0\n",
    "\n",
    "def z_clip(xs, b):\n",
    "    return [min(x, b) for x in xs]\n",
    "\n",
    "def g_clip(v):\n",
    "    n = np.linalg.norm(v, ord=2)\n",
    "    if n > 1:\n",
    "        return v / n\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 机器学习\n",
    "\n",
    "```{admonition} 学习目标\n",
    "阅读本章后，您将能够：\n",
    "- 描述和实现基础梯度下降算法\n",
    "- 使用高斯机制实现差分隐私梯度下降\n",
    "- 裁剪梯度，保证任意损失函数都可实现差分隐私保护\n",
    "- 描述噪声给训练过程带来的影响\n",
    "```\n",
    "\n",
    "本章我们将探索如何构建差分隐私学习分类器。我们将重点关注一类特定的*监督学习*问题：给定一组*带标签的训练样本*$\\{(x_1, y_1), \\dots, (x_n, y_n)\\}$，其中$x_i$称为*特征向量*，$y_i$称为*标签*，我们要训练一个*模型*$\\theta$。该模型可以*预测*没有在训练集中出现过的新特征向量所对应的标签。一般来说，每个$x_i$都是一个描述训练样本特征的实数向量，而$y_i$是从预先定义好的*类型*集合中选取的，每个类型一般用一个整数来表示。我们预先要从全部样本中提取出所有可能的类型，构成类型集合。一个二分类器的类型集合应包含两个类型（一般分别用1和0，或1和-1表示）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 使用Scikit-Learn实现逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "您可以从下述链接下载所需的数据集：\n",
    "\n",
    "- [`adult_processed_x`](https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_x.npy)\n",
    "- [`adult_processed_y`](https://github.com/uvm-plaid/programming-dp/raw/master/notebooks/adult_processed_y.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.load('adult_processed_x.npy')\n",
    "y = np.load('adult_processed_y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "当要训练一个模型时，我们从所有可用的数据中选择一些数据来构造一组*训练样本*（如前所述），但我们也会留出一些数据作为*测试样本*。一旦训练完模型，我们肯定想要知道该模型在*非*训练样本上的表现如何。如果一个模型在未知的新样本上表现很好，我们称其*泛化*能力很好。一个泛化能力*不足*的模型，我们称其在训练数据上发生了*过拟合*。\n",
    "\n",
    "我们使用测试样本来测试模型的泛化能力。由于我们事先已知测试样本的标签，我们可以让模型对每个测试样本进行分类，并比较预测标签和真实标签的结果，以测试模型的泛化能力。我们将把数据集切分为训练集和测试集。训练集包含80%的样本，而测试集包含其余20%的样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(9044,)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_size = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train = X[:training_size]\n",
    "X_test = X[training_size:]\n",
    "\n",
    "y_train = y[:training_size]\n",
    "y_test = y[training_size:]\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "构建一个二分类器的简单方法是使用*逻辑回归（Logistic Regression）*。scikit-learn库包含了一个实现逻辑回归的内置模块，名为`LogisticRegression`。通过调用此内置模块，很容易应用我们的数据构建二分类模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X_train[:1000],y_train[:1000])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来，我们可以使用模型的`predict`（预测）方法预测测试集的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1., -1., -1., ..., -1., -1., -1.])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们的模型预测对了多少个测试样本呢？我们可以比较预测标签和真实标签的结果。如果用预测正确的标签数量除以测试样本总数，我们就可以计算出测试样本的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8243034055727554"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.predict(X_test) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们的模型对测试样本的标签预测准确率为82%。对该数据集来说，这是一个相当不错的预测准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型是什么？\n",
    "\n",
    "模型到底*是*什么？它是如何编码预测所用信息的？\n",
    "\n",
    "有很多种不同类型的模型。这里我们要探讨的是*线性模型（Linear Model）*。给定一个包含$k$-维特征向量$x_1, \\dots, x_k$的无标签样本，线性模型预测此样本的标签时，将计算下述值：\n",
    "\n",
    "\\begin{align}\n",
    "w_1 x_1 + \\dots + w_k x_k + bias\n",
    "\\end{align}\n",
    "\n",
    "并用此值的符号作为预测标签（如果此值为负数，则预测结果为-1；如果此值为正数，则预测结果为1）。\n",
    "\n",
    "此模型可以用一个由$w_1, \\dots, w_k$和$bias$组成的向量来表示。之所以称该模型是线性模型，是因为模型在预测时要计算一个1次多项式（即线性多项式）的值。$w_1, \\dots, w_k$通常被称为模型的*权重*或*系数*，$bias$则被称为*偏差项*或*截距*。\n",
    "\n",
    "这实际上也是scikit-learn表示逻辑回归模型的方式！我们可以使用模型的`coef_`属性来查看训练得到的模型权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(-5.346167750305752,\n array([ 3.76035057e-01, -2.55358856e-01, -3.21341426e-02,  3.74545737e-01,\n        -6.85885223e-01,  3.91875239e-01, -1.69476241e-01, -7.41793527e-02,\n        -5.76496538e-01,  3.94976503e-01, -3.41457312e-01, -6.24912317e-01,\n        -6.05605602e-01, -4.56928100e-01, -5.19167009e-01, -1.05743009e-01,\n         8.19586633e-01,  9.96762702e-01, -3.09342985e-01,  6.57277160e-01,\n        -1.06436104e-01,  7.71287796e-01,  7.99791034e-02,  1.43803702e-01,\n        -1.01006564e-01,  1.59416785e+00, -5.06233997e-02, -5.78477239e-01,\n        -3.72601413e-01, -6.35661364e-01, -1.02810175e-01,  0.00000000e+00,\n        -1.35478173e-01,  4.36864993e-01, -3.42554362e-01, -1.32819675e-01,\n        -2.00200285e-01, -1.53919241e+00,  6.44831702e-02,  7.17836796e-01,\n         3.80039408e-01,  4.25898498e-02,  8.81653483e-01, -7.08110462e-02,\n         6.10385977e-02,  8.94590966e-02,  6.93679716e-01, -1.30382712e+00,\n        -6.55878656e-01,  1.11512993e+00,  3.78012650e-01, -4.28231712e-02,\n        -3.72812689e-01,  2.41180415e-01, -2.03955636e-01, -3.07042908e-01,\n         3.06644477e-01,  4.31360344e-01,  5.31199745e-01, -6.89615763e-02,\n         4.66366585e-01, -5.81829004e-01, -2.21952424e-01, -2.39529124e-01,\n        -1.40562769e-03,  7.26045748e-01,  2.46167426e-01, -6.08617054e-01,\n         0.00000000e+00, -9.02427102e-02, -3.54430134e-03,  0.00000000e+00,\n         0.00000000e+00, -1.29034794e-01,  5.90856998e-01, -5.15912614e-01,\n         0.00000000e+00, -5.42096249e-03,  7.28556009e-01, -5.15261422e-02,\n         2.30704112e-01, -1.61821068e-01, -6.60183260e-01, -1.01170807e-01,\n        -2.52337853e-01, -5.77230791e-02, -1.45064565e-01, -3.09985224e-01,\n         0.00000000e+00, -3.31415590e-02,  0.00000000e+00, -1.38495395e-01,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.26243747e-01,\n         0.00000000e+00,  0.00000000e+00,  1.72867533e+00,  7.37281346e-02,\n         1.83154145e+00,  2.40009511e+00,  1.46921214e+00,  1.96856497e+00]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_[0], model.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "注意到，权重$w_i$的数量和特征$x_i$的数量总是一致的，因为模型在预测时需要将各个特征和其对应的权重相乘。这也意味着我们模型的维度和特征向量的维度完全相同。\n",
    "\n",
    "有了获得权重和偏差项的方法后，我们就可以实现自己的预测函数了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8243034055727554"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测：以模型（theta）单一样本（xi）为输入，返回预测标签\n",
    "def predict(xi, theta, bias=0):\n",
    "    label = np.sign(xi @ theta + bias)\n",
    "    return label\n",
    "\n",
    "np.sum(predict(X_test, model.coef_[0], model.intercept_[0]) == y_test)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这里我们将偏差项设置为可选参数，因为在大多数情况下，无偏差项模型的预测效果也足够好了。为了简化模型训练的整个过程，后续我们先不考虑训练偏差项。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 使用梯度下降训练模型\n",
    "\n",
    "训练过程究竟是如何进行的呢？scikit-learn库实现了一些非常复杂的算法，但我们也可以通过实现一个简单算法来实现同等的效果。此算法称为*梯度下降*（Gradient Descent）。\n",
    "\n",
    "大多数机器学习训练算法要根据所选择的*损失函数*（Loss Function）来定义。损失函数是一种衡量模型预测结果有多\"差\"的方法。训练算法的目标是使损失函数达到最小值。换句话说，损失值低的模型具有*更好的*预测能力。\n",
    "\n",
    "机器学习社区已经提出了多种不同的常用损失函数。对于每个预测正确的样本，简单的损失函数直接返回0。对于每个预测错误的样本，损失函数直接返回1。损失值为0意味着模型可以正确预测出每个样本的标签。二分类器中较为常用的损失函数为*对率损失*（Logistic Loss）。对率损失帮助我们度量出模型\"还有多远的距离\"才能正确预测出标签（与简单地输出0和1相比，对率损失可以提供更多的信息）。\n",
    "\n",
    "下述Python代码实现了对率损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 损失函数用于衡量我们的模型有多好。训练目标是最小化损失值。\n",
    "# 这是对率损失函数。\n",
    "def loss(theta, xi, yi):\n",
    "    exponent = - yi * (xi.dot(theta))\n",
    "    return np.log(1 + np.exp(exponent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们可以使用损失函数衡量指定模型的效果。让我们用权重全为0的模型来试一试。该模型大概率效果不佳，但我们可以把此模型作为起点，逐步训练出更好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6931471805599453"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.zeros(X_train.shape[1])\n",
    "loss(theta, X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "一般来说，通过简单地计算所有训练样本的平均损失值，我们就可以测量出模型在整个训练集上有多好。当模型的权重全为0时，所有样本*全部*预测错误，整个训练集的平均损失值刚好等于我们前面计算得到的单个样本损失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.6931471805599453"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([loss(theta, x_i, y_i) for x_i, y_i in zip(X_train, y_train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们*训练*模型的目标是*最小化*损失值。这里的关键问题是：我们如何修改模型才能降低损失值呢？\n",
    "\n",
    "梯度下降是一种根据[*梯度*](https://en.wikipedia.org/wiki/Gradient)更新模型以降低损失值的方法。梯度就像一个多维导数：对于一个有着多维输入的函数（例如我们前面提到的损失函数），梯度告诉我们*每个*维度输入的变化会在多大程度上影响函数输出的变化。如果某一维度的梯度为正，意味着一旦我们增加该维度的模型权重，函数输出值将*变大*。我们想要*降低*损失值，因此我们应该用*负*梯度来修改模型，即做与梯度*相反*的事情。由于我们沿梯度的相反方向修改模型，因此这一过程称为*降低*梯度。\n",
    "\n",
    "当我们经过多次迭代，重复执行此下降过程后，我们会越来越接近最小化损失值的模型。这就是*梯度下降*的整个过程。让我们来看看梯度下降算法在Python下的运行效果。首先，我们定义梯度函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 这是对率损失函数的梯度\n",
    "# 梯度是一个表示各个方向损失变化率的向量\n",
    "def gradient(theta, xi, yi):\n",
    "    exponent = yi * (xi.dot(theta))\n",
    "    return - (yi*xi) / (1+np.exp(exponent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 单步梯度下降\n",
    "\n",
    "接下来，我们来单步执行一次梯度下降。我们将训练集中的一个样本都输入到`gradient`（梯度）函数中，得到此样本的梯度值。得到梯度值可以为我们提供足够信息来改善模型。我们在当前模型`theta`上减去得到的梯度，以实现梯度\"下降\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.        ,  0.        ,  0.        ,  0.        , -0.5       ,\n        0.        ,  0.        ,  0.        , -0.5       ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n       -0.5       ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        , -0.5       ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        , -0.5       ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        , -0.5       ,\n        0.        , -0.5       ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n       -0.5       ,  0.        ,  0.        , -0.25      , -0.0606146 ,\n       -0.21875   ,  0.        ,  0.        , -0.17676768])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果我们想把我们向梯度的反方向移动一步（即减去梯度值），\n",
    "# 我们应该可以让theta向损失值*变小）的方向移动\n",
    "# 这就是单步梯度下降。我们在每一步都要尝试*降低*梯度\n",
    "# 在这个例子中，我们只计算了训练集中（第）一个样本的梯度\n",
    "theta = theta - gradient(theta, X_train[0], y_train[0])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "现在，如果我们以相同的训练样本为输入调用`predict`函数，模型就可以正确预测此样本的标签了！我们的模型更新方法确实提高了模型的预测能力，因为更新后的模型已经具备分类此样本的能力了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(-1.0, -1.0)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], predict(theta, X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们需要多次度量模型的准确性。为此，我们定义一个用于度量准确性的辅助函数。它的工作方式和sklearn模型的准确性度量方式相同。我们可以用这个函数度量经过单个样本梯度下降后所得到的模型`theta`，看看新模型在测试集的效果怎么样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.7585139318885449"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(theta):\n",
    "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
    "\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们改善后的模型现在可以正确预测测试集中75%的标签！这是一个很大的进步，我们大大改善了模型的预测效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 梯度下降算法\n",
    "\n",
    "我们需要进一步对算法的两个部分进行改进，从而最终实现基础梯度下降算法。第一，我们前面的单步梯度下降仅使用了训练数据中的单个样本。我们希望用*整个*训练集来更新模型，从而改善模型在*所有*样本上的预测效果。第二，我们需要执行多次迭代，尽可能让损失值达到最小。\n",
    "\n",
    "对于第一个改进点，我们可以计算所有训练样本的*平均梯度*，以替代单步梯度下降中的单样本梯度。我们用下述实现的`avg_grad`（平均梯度）函数来计算所有训练样本和对应标签的平均梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide-output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([-8.03202480e-03, -1.09365062e-02, -5.86649848e-02, -1.70297784e-02,\n       -1.85949049e-02, -5.32762100e-03,  3.15432083e-05,  2.24692568e-03,\n        1.80942171e-03,  1.10891317e-03,  7.17940863e-04,  1.22012681e-03,\n        1.09385854e-03,  1.42352970e-03, -4.29266203e-03, -5.73114012e-03,\n       -4.96409990e-02, -7.90844879e-03, -1.08970068e-02, -2.50609905e-02,\n        3.27410319e-04, -1.20102580e-02, -1.29608985e-02,  1.15182321e-02,\n       -2.26895536e-04, -1.83255483e-01,  1.34642262e-03,  4.47703452e-02,\n        4.31895523e-03,  2.97414610e-03,  6.16295082e-03, -4.88903955e-05,\n       -2.13933205e-02, -4.86969833e-02, -8.62802483e-04,  3.11463168e-03,\n        1.23013848e-03,  1.54486498e-02,  1.21336873e-03, -4.38864985e-02,\n       -4.34689131e-03, -1.64743409e-02, -4.53583200e-03, -5.47845717e-03,\n       -1.67472715e-01,  1.93015718e-02,  4.73608091e-03,  2.44149704e-02,\n        1.61917788e-02, -1.57259641e-02,  6.59058497e-04, -1.58429762e-03,\n        9.21938268e-03,  8.76978910e-04, -1.27725399e-01,  3.39811988e-02,\n       -1.52535476e-01, -1.11859092e-04, -7.43481028e-04, -2.46346175e-04,\n        2.71911076e-04, -2.55366711e-04,  4.50825450e-04,  1.10378277e-04,\n        3.56606530e-04, -6.45268003e-04, -2.29994332e-04, -3.86436617e-04,\n       -3.08625397e-04,  2.96102401e-04,  1.88227302e-04,  8.58078928e-06,\n        7.20867325e-05, -4.19942412e-05, -8.78083803e-05, -8.39666492e-04,\n       -3.06575834e-04, -8.40712924e-05, -5.70563641e-04,  4.00302057e-04,\n       -2.64158094e-04,  6.99057157e-05,  2.42709304e-03,  1.82470777e-04,\n        8.76079931e-05,  1.54645694e-04, -2.72063515e-04, -6.37207436e-05,\n        1.24980547e-05,  4.45197135e-04,  4.61621071e-05,  1.15265174e-04,\n       -2.77439358e-04,  5.96595409e-05,  1.20539191e-04, -1.18965672e-01,\n        3.44932395e-04, -7.41634269e-05, -6.91870325e-02, -1.45516103e-02,\n       -9.95735544e-02, -8.85669054e-03, -9.10018120e-03, -6.35462985e-02])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_grad(theta, X, y):\n",
    "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
    "    return np.mean(grads, axis=0)\n",
    "\n",
    "avg_grad(theta, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "对于第二个改进点，我们来定义一个可以多次执行梯度下降的迭代算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(iterations):\n",
    "    # 我们用\"猜测\"的一个模型参数（权重全为0的模型）作为起始点\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    # 应用训练集迭代执行梯度下降步骤\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.7787483414418399"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = gradient_descent(10)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "经过10轮迭代，我们的模型几乎达到78%的准确率，效果还不错！我们的梯度下降算法看起来很简单（也确实挺简单的！），但这个算法可以说是大智若愚：该方法是近年来绝大多数大规模深度学习的基础。我们给出的算法在设计上已经非常接近于TensorFlow等主流机器学习框架所实现的算法了。\n",
    "\n",
    "注意到，与前面用sklearn训练得到的模型相比，我们的模型还没有达到84%的准确率。别担心，我们的算法绝对有能力做到！我们只是需要更多轮迭代，使损失值更接近最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "经过100轮迭代，模型的准确率达到82%，更接近84%的准确率了。但是，如此多的迭代次数导致算法运行了非常长的时间。更糟糕的是，我们越接近最小损失值，模型的预测效果就越难得到进一步的改善了。100轮迭代后的模型可以达到82%的准确率，但达到84%的准确率可能需要1000轮迭代。这也体现出了机器学习的一个根本矛盾：一般来说，更多的训练轮数可以带来更高的准确率，但同时也需要更多的计算时间。在实际场景中使用大规模深度学习时，绝大多数实现\"技巧\"都是为梯度下降的每轮迭代加速，以便在相同时间内执行更多轮迭代。\n",
    "\n",
    "还有一个有趣的现象值得我们注意：损失函数的输出值确实会随着每轮梯度下降的迭代而下降。因此，随着执行轮数的增加，我们的模型的确在逐渐接近最小损失值。另外要注意的是，如果训练集和测试集的损失值非常接近，意味着我们的模型没有*过拟合*训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.549109439168421\n",
      "Testing loss: 0.5415350837580458\n",
      "\n",
      "Training loss: 0.5224689105514977\n",
      "Testing loss: 0.5162665121068426\n",
      "\n",
      "Training loss: 0.5028090736020403\n",
      "Testing loss: 0.49753785424732383\n",
      "\n",
      "Training loss: 0.4878874803989895\n",
      "Testing loss: 0.48335633696635527\n",
      "\n",
      "Training loss: 0.47628573924997936\n",
      "Testing loss: 0.4723742456095848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_log(iterations):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        theta = theta - avg_grad(theta, X_train, y_train)\n",
    "        print(f'训练集损失值: {np.mean(loss(theta, X_train, y_train))}')\n",
    "        print(f'测试集损失值: {np.mean(loss(theta, X_test, y_test))}\\n')\n",
    "\n",
    "    return theta\n",
    "\n",
    "gradient_descent_log(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 差分隐私梯度下降\n",
    "\n",
    "我们如何使上述算法满足差分隐私呢？我们想要设计一种算法来为训练数据提供差分隐私保护，使最终训练得到的模型不会泄露与单个训练样本相关的任何信息。\n",
    "\n",
    "算法执行过程中唯一使用了训练数据的部分是梯度计算步骤。使该算法满足差分隐私的一种方法是，在每轮模型更新前在梯度上增加噪声。由于我们直接在梯度上增加噪声，因此该方法通常被称为*噪声梯度下降*（Noisy Gradient Descent）。\n",
    "\n",
    "我们的梯度函数是向量值函数，因此我们使用`gaussian_mech_vec`（向量高斯机制）在梯度函数的输出值上增加噪声："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    sensitivity = '???'\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad = avg_grad(theta, X_train, y_train)\n",
    "        noisy_grad = gaussian_mech_vec(grad, sensitivity, epsilon, delta)\n",
    "        theta = theta - noisy_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "这里就差一块拼图了：**梯度函数的敏感度是多少**？这是使算法满足差分隐私的关键所在。\n",
    "\n",
    "这里我们主要面临两个挑战。第一，梯度是*均值问询*的结果，即梯度是每个样本梯度的均值。我们之前已经提到，最好将均值问询拆分为一个求和问询和一个计数问询。做到这一点并不难，我们可以不直接计算梯度均值，而是计算每个样本梯度噪声和，再除以噪声计数值。第二，我们需要限制每个样本梯度的敏感度。有两种基础方法可以做到这一点。我们可以（如之前讲解的其他问询那样）分析梯度函数，确定其在最差情况下的全局敏感度。我们也可以（如\"采样-聚合\"框架那样）裁剪梯度函数的输出值，从而*强制*限定敏感度上界。\n",
    "\n",
    "我们先介绍第二种方法。第二种方法从概念上看更简单，在实际应用中的普适性更好。此方法一般被称为*梯度裁剪*（Gradient Clipping）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 梯度裁剪\n",
    "\n",
    "回想一下，在实现\"采样-聚合\"框架时，我们裁剪未知敏感度函数$f$的输出，强制限定$f$的敏感度上界。$f$的敏感度为：\n",
    "\n",
    "\\begin{align}\n",
    "\\lvert f(x) - f(x') \\rvert\n",
    "\\end{align}\n",
    "\n",
    "使用参数$b$裁剪后，上述表达式变为：\n",
    "\n",
    "\\begin{align}\n",
    "\\lvert \\mathsf{clip}(f(x), b) - \\mathsf{clip}(f(x'),b) \\rvert\n",
    "\\end{align}\n",
    "\n",
    "最差情况下，$\\mathsf{clip}(f(x), b) = b$，且$\\mathsf{clip}(f(x'),b) = 0$，因此裁剪结果的敏感度为$b$（即敏感度等于裁剪参数）。\n",
    "\n",
    "我们可以使用相同的技巧来限定梯度函数的$L2$敏感度。我们需要定义一个用来\"裁剪\"向量的函数，使输出向量的$L2$范数落在期望的范围内。我们可以通过*缩放*向量来做到这一点：如果把向量中每个位置的元素都除以向量的$L2$范数，则所得向量的$L2$范数为1。如果想要使用裁剪参数$b$，我们可以在缩放后的向量上乘以$b$，将其放大回$L2$范数等于$b$的向量。我们还希望不对$L2$范数已经小于$b$的向量进行任何修改。因此，如果向量的$L2$范数已经小于$b$，我们直接返回此向量即可。我们可以使用`np.linalg.norm`函数，并以参数`ord=2`作为输入，以计算向量的$L2$范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def L2_clip(v, b):\n",
    "    norm = np.linalg.norm(v, ord=2)\n",
    "    \n",
    "    if norm > b:\n",
    "        return b * (v / norm)\n",
    "    else:\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "现在，我们可以开始分析裁剪梯度的敏感度了。我们将梯度表示为$\\nabla(\\theta; X, y)$（对应我们Python代码中的`gradient`）：\n",
    "\n",
    "\\begin{align}\n",
    "\\lVert \\mathsf{L2\\_clip}( \\nabla (\\theta; X, y), b) - \\mathsf{L2\\_clip}( \\nabla (\\theta; X', y), 0) \\rVert_2\n",
    "\\end{align}\n",
    "\n",
    "最差情况下，$\\mathsf{L2\\_clip}( \\nabla (\\theta; X, y), b)$的$L2$范数为$b$，且$\\mathsf{L2\\_clip}( \\nabla (\\theta; X', y))$全为0。此时，两者的$L2$范数差等于$b$。这样一来，我们成功用裁剪参数$b$限定了梯度的$L2$敏感度上界！\n",
    "\n",
    "现在，我们可以继续计算裁剪梯度之和，并根据我们通过裁剪技术得到的$L2$敏感度上界$b$来增加噪声。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
    "        \n",
    "    # 求和问询\n",
    "    # （经过裁剪后的）L2敏感度为b\n",
    "    return np.sum(gradients, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们现在就快要完成噪声梯度下降算法的设计和实现了。为了计算平均噪声梯度，我们需要：\n",
    "\n",
    "1. 基于敏感度$b$，在梯度和上增加噪声\n",
    "2. 计算训练样本数量的噪声计数值（敏感度为1）\n",
    "3. 用(1)的噪声梯度值和除以(2)的噪声计数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    sensitivity = 5.0\n",
    "    \n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad_sum        = gradient_sum(theta, X_train, y_train, sensitivity)\n",
    "        noisy_grad_sum  = gaussian_mech_vec(grad_sum, sensitivity, epsilon, delta)\n",
    "        noisy_avg_grad  = noisy_grad_sum / noisy_count\n",
    "        theta           = theta - noisy_avg_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.777421494913755"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = noisy_gradient_descent(10, 0.1, 1e-5)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "该算法的每轮迭代过程都满足$(\\epsilon, \\delta)$-差分隐私。我们还需额外执行一次噪声计数问询来得到满足$\\epsilon$-差分隐私的噪声计数值。如果执行$k$轮迭代，则根据串行组合性，算法满足$(k\\epsilon + \\epsilon, k\\delta)$-差分隐私。我们也可以使用高级组合性来分析总隐私消耗量。更进一步，我们可以将算法转化为瑞丽差分隐私或零集中差分隐私，应用相应的组合定理得到更紧致的总隐私消耗量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 梯度的敏感度\n",
    "\n",
    "前面所述方法的普适性很高，因为此方法不需要假设梯度函数满足什么特定的要求。但是，我们有时*的确*对梯度函数有所了解。特别地，一大类常用的梯度函数（包括本章用到的对率损失梯度）是*利普希茨连续*（Lipschitz Continuous）的。这意味着这些梯度函数的全局敏感度是有界的。用数学语言描述，我们可以证明：\n",
    "\n",
    "\\begin{align}\n",
    "\\text{If}\\; \\lVert x_i \\rVert_2 \\leq b\\; \\text{then}\\; \\lVert \\nabla(\\theta; x_i, y_i) \\rVert_2 \\leq b\n",
    "\\end{align}\n",
    "\n",
    "这一结论允许我们通过裁剪*训练样本*（即梯度函数的*输入*）来获得梯度函数的$L2$敏感度上界。这样，我们就不再需要裁剪梯度函数的*输出*了。\n",
    "\n",
    "用裁剪训练样本代替裁剪梯度会带来两个优点。第一，与预估训练阶段的梯度尺度相比，预估训练样本的尺度（进而选择一个好的裁剪参数）通常要容易得多。第二，裁剪训练样本的计算开销更低：我们只需要对训练样本裁剪一次，训练模型时就可以重复使用裁剪后的训练数据了。但如果选择裁剪梯度，我们就需要裁剪训练过程中计算得到的每一个梯度。此外，为了实现梯度裁剪，我们不得不依次计算出每个训练样本的梯度。但如果选择裁剪训练样本，我们就可以一次计算得到所有训练样本的梯度，从而提高训练效率（这是机器学习中的常用技巧，这里我们不再展开讨论）。\n",
    "\n",
    "然而，需要注意的是，还有很多常用损失函数的全局敏感度是无界的，尤其是深度学习中神经网络里用到的损失函数更是如此。对于这些损失函数，我们只能使用梯度裁剪法。\n",
    "\n",
    "我们只需对算法进行简单的修改，就可以把裁剪梯度替换为裁剪训练样本。在开始训练之前，我们需要先使用`L2_clip`（L2裁剪）函数来裁剪训练样本。随后，我们只需要直接把裁剪梯度的代码移除即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_sum(theta, X, y, b):\n",
    "    gradients = [gradient(theta, x_i, y_i) for x_i, y_i in zip(X,y)]\n",
    "\n",
    "    # 求和问询\n",
    "    # （经过裁剪后的）L2敏感度为b\n",
    "    return np.sum(gradients, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def noisy_gradient_descent(iterations, epsilon, delta):\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    sensitivity = 5.0\n",
    "    \n",
    "    noisy_count = laplace_mech(X_train.shape[0], 1, epsilon)\n",
    "    clipped_X = [L2_clip(x_i, sensitivity) for x_i in X_train]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        grad_sum        = gradient_sum(theta, clipped_X, y_train, sensitivity)\n",
    "        noisy_grad_sum  = gaussian_mech_vec(grad_sum, sensitivity, epsilon, delta)\n",
    "        noisy_avg_grad  = noisy_grad_sum / noisy_count\n",
    "        theta           = theta - noisy_avg_grad\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.7791906236178682"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = noisy_gradient_descent(10, 0.1, 1e-5)\n",
    "accuracy(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "M可以对此算法进行多种改进，以进一步降低隐私消耗量、提升模型预测的准确率。很多改进方法都源自机器学习领域的论文。这里给出几个例子：\n",
    "\n",
    "- 将总隐私消耗量限定为$\\epsilon$，在算法内部计算每轮迭代的隐私消耗量$\\epsilon_i$。\n",
    "- 利用高级组合性、瑞丽差分隐私或零集中差分隐私，从而获得更好的总隐私消耗量。\n",
    "- 小批次训练：每轮迭代中，不使用整个训练数据，而是使用一小块训练数据来计算梯度（这样可以减少梯度计算过程中的计算开销）。\n",
    "- 同时使用小批次训练和并行组合性。\n",
    "- 同时使用小批次训练和小批次随机采样。\n",
    "- 调整学习率$\\eta$等其他超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 噪声对训练的影响\n",
    "\n",
    "我们已经知道，迭代次数会对模型的预测准确率带来很大的影响，因为更多的迭代次数可以使模型更接近最小损失值。我们的差分隐私算法需要在梯度上增加噪声，这会对准确率噪声带来很大的影响。噪声可能导致训练算法在训练过程中向*错误的方向*移动，使模型变得*更糟糕*。\n",
    "\n",
    "我们有理由相信更小的$\\epsilon$会带来准确率更低的模型（我们已经学习的差分隐私算法都存在类似的关系）。这个结论确实是正确的，但由于在执行多轮迭代算法时要考虑组合定理，因此这里也存在一些微妙的平衡。更多的迭代次数意味着更大的隐私消耗量，而在标准梯度下降算法中，更多的迭代次数一般意味着产出更好的模型。在差分隐私保护下，当总隐私消耗量保持不变时，更多的迭代次数可能会导致模型变得更加糟糕，因为我们不得不使用更小的$\\epsilon$来支持更多轮迭代，这会带来更大的噪声。在差分隐私机器学习中，适当平衡迭代轮数和单轮添加的噪声量是一个很重要的（有时也是一个非常有挑战性的）问题。\n",
    "\n",
    "让我们做一个小实验，看看不同的$\\epsilon$会对模型的预测准确率带来何种影响。我们将使用不同的$\\epsilon$来训练模型，每次训练迭代20轮。我们根据训练时使用的$\\epsilon$作为横坐标来绘制每个模型的准确率变化图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "hide-cell"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "delta = 1e-5\n",
    "\n",
    "epsilons = [0.001, 0.003, 0.005, 0.008, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "thetas   = [noisy_gradient_descent(10, epsilon, delta) for epsilon in epsilons]\n",
    "accs     = [accuracy(theta) for theta in thetas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "hide-input"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('ε')\n",
    "plt.ylabel('准确率')\n",
    "plt.plot(epsilons, accs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "从上图可以看出，$\\epsilon$非常小时会产生准确率非常低的模型。请记住，我们在图中指定的$\\epsilon$是每轮迭代时使用的$\\epsilon$，因此组合后的总隐私消耗量还要大得多。"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}